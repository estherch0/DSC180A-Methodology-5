Esther Cho
ehcho@ucsd.edu

**1. What is the most interesting topic covered in your domain this quarter?**
The most interesting topic we have covered is Graph Transformers. These models process graph-structured data by extending transformers, which were first created for sequential data, such as text or music. Unlike Graph Neural Networks, Graph Transformers can model both the neighboring and global dependencies in a graph, which makes them practical for complex tasks. Their positional encodings, such as Laplacian Eigenmaps, add structural context to the nodes. This opens doors for applications in molecular biology (atoms and bonds), social networks, and recommendation systems.
**2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.**
For Quarter 2, I’d like to explore using Graph Transformers with Knowledge Graphs. Knowledge Graphs are effective in illustrating the connections between numerous entities across a wide range of domains. For instance, recommendation and semantic search systems. The project can use Graph Transformers to better model multi-hop relationships and contact in knowledge graphs, using Graph Transformer’s ability to use attention mechanisms to highlight essential connections. I think a good idea would be to investigate how Graph Transformers could enhance recommendation systems - maybe representing users’ preferences or items’ attributes as nodes and their interactions as edges. This project could explore applications in personalized learning or streaming platforms.
**3. What is a potential change you’d make to the approach taken in your current Quarter 1 Project?**
One potential change I’d make to the approach taken in my current Quarter 1 project is to incorporate Knowledge Graphs as a whole. Adding a Knowledge Graph could allow the model to account for relationships and attributes beyond direct interactions. This would leverage Graph Transformers ability to learn complex dependencies in graphs. The change could improve personalization and interpretability compared to more straightforward graph representations.
**4. What other techniques would you be interested in using in your project?**
For my project, I would like to investigate the KnowFormer approach, which employs graph transformers for Knowledge Graph. This approach incorporates structural information into the attention mechanism to improve reasoning through structure-aware modules. Unlike regular Graph Neural Networks, it addresses missing paths and information over-squashing. Additionally, I would like to investigate how KnowFormer can be scaled and optimized for more complex knowledge graphs, especially in domains such as personalized recommendation systems, where Knowledge Graph reasoning is important.
